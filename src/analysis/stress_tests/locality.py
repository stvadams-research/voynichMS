"""
Track B3: Locality and Compositionality Tests

Determines whether structure is local, global, compositional, or procedural.

Key questions:
- Do mappings depend on local neighborhoods or global context?
- Is structure compositional (parts combine predictably)?
- Is structure procedural (generated by rules)?
"""

from typing import List, Dict, Any
import math
from collections import Counter

from analysis.stress_tests.interface import (
    StressTest,
    StressTestResult,
    StressTestOutcome,
)
from foundation.storage.metadata import (
    MetadataStore,
    PageRecord,
    WordRecord,
    LineRecord,
    GlyphCandidateRecord,
    GlyphAlignmentRecord,
    RegionRecord,
    RegionEdgeRecord,
    TranscriptionTokenRecord,
    TranscriptionLineRecord,
    WordAlignmentRecord,
)
from foundation.config import use_real_computation


# Iteration limits for bounded runtime on large datasets
MAX_PAGES_PER_TEST = 50  # Maximum pages to analyze per test
MAX_LINES_PER_PAGE = 100  # Maximum lines to analyze per page
MAX_WORDS_PER_LINE = 50  # Maximum words to analyze per line
MAX_TOKENS_ANALYZED = 10000  # Maximum tokens to analyze for entropy calculations


class LocalityTest(StressTest):
    """
    Tests locality and compositionality of manuscript structure.

    Evaluates:
    - Local vs global dependency
    - Compositional structure
    - Procedural generation signatures
    """

    @property
    def test_id(self) -> str:
        return "locality_compositionality"

    @property
    def description(self) -> str:
        return (
            "Tests whether structure depends on local neighborhoods, "
            "global context, or exhibits compositional/procedural patterns."
        )

    @property
    def applicable_classes(self) -> List[str]:
        return ["constructed_system", "visual_grammar", "hybrid_system"]

    def run(self, explanation_class: str, dataset_id: str,
            control_ids: List[str]) -> StressTestResult:
        """
        Execute locality and compositionality tests.

        Tests:
        1. Locality radius (how far do dependencies extend?)
        2. Compositional patterns (do parts combine predictably?)
        3. Procedural signatures (generated vs organic patterns?)
        """
        session = self.store.Session()
        try:
            # Test locality
            locality_result = self._test_locality(session, dataset_id)

            # Test compositionality
            compositionality_result = self._test_compositionality(session, dataset_id)

            # Test procedural signatures
            procedural_result = self._test_procedural_signatures(session, dataset_id)

            # Compare to controls
            control_results = {}
            for ctrl_id in control_ids:
                control_results[ctrl_id] = {
                    "locality": self._test_locality(session, ctrl_id),
                    "compositionality": self._test_compositionality(session, ctrl_id),
                    "procedural": self._test_procedural_signatures(session, ctrl_id),
                }

            # Aggregate analysis
            analysis = self._analyze_results(
                explanation_class,
                locality_result,
                compositionality_result,
                procedural_result,
                control_results
            )

            # Determine outcome
            outcome = self._determine_outcome(explanation_class, analysis)

            return StressTestResult(
                test_id=self.test_id,
                explanation_class=explanation_class,
                outcome=outcome,
                stability_score=analysis.get("overall_score", 0.5),
                control_differential=analysis.get("control_differential", 0),
                collapse_threshold=None,
                metrics={
                    "locality_radius": locality_result.get("radius", 0),
                    "local_dependency_strength": locality_result.get("strength", 0),
                    "compositionality_score": compositionality_result.get("score", 0),
                    "procedural_signature": procedural_result.get("signature_strength", 0),
                    "pattern_type": analysis.get("pattern_type", "unknown"),
                },
                failure_cases=analysis.get("failures", []),
                tightens_constraints=analysis.get("tightens_constraints", False),
                rules_out_class=outcome == StressTestOutcome.COLLAPSED,
                constraint_implications=analysis.get("implications", []),
                evidence_refs=["geometric_anchors", "glyph_position_entropy"],
                summary=self._generate_summary(explanation_class, outcome, analysis)
            )

        finally:
            session.close()

    def _test_locality(self, session, dataset_id: str) -> Dict[str, Any]:
        """
        Test locality of structural dependencies.

        Measures how far structural influence extends.
        Uses iteration limits for bounded runtime on large datasets.
        """
        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()

        if not pages:
            return {"radius": 0, "strength": 0}

        # Analyze word-level locality
        local_correlations = []
        global_correlations = []

        for page in pages:
            lines = session.query(LineRecord).filter_by(page_id=page.id).limit(MAX_LINES_PER_PAGE).all()

            for line in lines:
                words = session.query(WordRecord).filter_by(line_id=line.id).limit(MAX_WORDS_PER_LINE).all()
                if len(words) < 4:
                    continue

                # Local correlation: adjacent words
                local_sim = self._calculate_local_similarity(session, words, dataset_id)
                local_correlations.append(local_sim)

                # Global correlation: first vs last word
                global_sim = self._calculate_global_similarity(session, words, dataset_id)
                global_correlations.append(global_sim)

        avg_local = sum(local_correlations) / len(local_correlations) if local_correlations else 0
        avg_global = sum(global_correlations) / len(global_correlations) if global_correlations else 0

        # Locality ratio: local >> global suggests local structure
        locality_ratio = avg_local / (avg_global + 0.01)

        # Estimate effective radius (how many words away influence extends)
        if locality_ratio > 2.0:
            radius = 2  # Very local (2-word context)
        elif locality_ratio > 1.5:
            radius = 4  # Moderately local
        elif locality_ratio > 1.0:
            radius = 8  # Wider context
        else:
            radius = -1  # Global dependencies dominate

        return {
            "radius": radius,
            "strength": avg_local,
            "local_avg": avg_local,
            "global_avg": avg_global,
            "locality_ratio": locality_ratio,
        }

    def _calculate_local_similarity(self, session, words: List[WordRecord], dataset_id: str) -> float:
        """
        Calculate local (adjacent) similarity between words.

        Uses bigram transition probabilities from actual tokens.
        """
        if not use_real_computation("stress_tests"):
            return 0.65  # Simulated

        if len(words) < 2:
            return 0.0

        # Get tokens for each word via alignments
        word_tokens = []
        for word in words:
            alignment = (
                session.query(WordAlignmentRecord)
                .filter_by(word_id=word.id)
                .first()
            )
            if alignment and alignment.token_id:
                token = session.query(TranscriptionTokenRecord).filter_by(id=alignment.token_id).first()
                if token:
                    word_tokens.append(token.content)
                else:
                    word_tokens.append(None)
            else:
                word_tokens.append(None)

        # Filter out None tokens
        valid_tokens = [t for t in word_tokens if t is not None]

        if len(valid_tokens) < 2:
            return 0.0

        # Calculate bigram overlap (how often adjacent tokens share characters)
        adjacent_similarities = []
        for i in range(len(valid_tokens) - 1):
            t1, t2 = valid_tokens[i], valid_tokens[i + 1]

            # Character-level Jaccard similarity
            chars1 = set(t1)
            chars2 = set(t2)

            if chars1 or chars2:
                jaccard = len(chars1 & chars2) / len(chars1 | chars2)
                adjacent_similarities.append(jaccard)

        return sum(adjacent_similarities) / len(adjacent_similarities) if adjacent_similarities else 0.0

    def _calculate_global_similarity(self, session, words: List[WordRecord], dataset_id: str) -> float:
        """
        Calculate global (distant) similarity between words.

        Compares first and last tokens in the line.
        """
        if not use_real_computation("stress_tests"):
            return 0.35  # Simulated

        if len(words) < 4:
            return 0.0

        # Get tokens for first and last quarter of words
        first_quarter = words[:len(words) // 4]
        last_quarter = words[-(len(words) // 4):]

        first_tokens = []
        last_tokens = []

        for word in first_quarter:
            alignment = session.query(WordAlignmentRecord).filter_by(word_id=word.id).first()
            if alignment and alignment.token_id:
                token = session.query(TranscriptionTokenRecord).filter_by(id=alignment.token_id).first()
                if token:
                    first_tokens.append(token.content)

        for word in last_quarter:
            alignment = session.query(WordAlignmentRecord).filter_by(word_id=word.id).first()
            if alignment and alignment.token_id:
                token = session.query(TranscriptionTokenRecord).filter_by(id=alignment.token_id).first()
                if token:
                    last_tokens.append(token.content)

        if not first_tokens or not last_tokens:
            return 0.0

        # Token-level Jaccard similarity
        first_set = set(first_tokens)
        last_set = set(last_tokens)

        if first_set or last_set:
            return len(first_set & last_set) / len(first_set | last_set)

        return 0.0

    def _test_compositionality(self, session, dataset_id: str) -> Dict[str, Any]:
        """
        Test compositionality of structure.

        Do parts combine in predictable ways?
        Uses iteration limits for bounded runtime.
        """
        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()

        if not pages:
            return {"score": 0, "type": "unknown"}

        # Analyze glyph-to-word compositionality
        composition_scores = []

        for page in pages:
            lines = session.query(LineRecord).filter_by(page_id=page.id).limit(MAX_LINES_PER_PAGE).all()

            for line in lines:
                words = session.query(WordRecord).filter_by(line_id=line.id).limit(MAX_WORDS_PER_LINE).all()

                for word in words:
                    glyphs = session.query(GlyphCandidateRecord).filter_by(word_id=word.id).all()
                    if len(glyphs) < 2:
                        continue

                    # Test: do glyph combinations follow patterns?
                    score = self._analyze_glyph_composition(session, glyphs, dataset_id)
                    composition_scores.append(score)

        avg_score = sum(composition_scores) / len(composition_scores) if composition_scores else 0

        # Determine composition type
        if avg_score > 0.7:
            comp_type = "strongly_compositional"
        elif avg_score > 0.5:
            comp_type = "weakly_compositional"
        elif avg_score > 0.3:
            comp_type = "partially_compositional"
        else:
            comp_type = "non_compositional"

        return {
            "score": avg_score,
            "type": comp_type,
            "sample_size": len(composition_scores),
        }

    def _analyze_glyph_composition(self, session, glyphs: List[GlyphCandidateRecord], dataset_id: str) -> float:
        """
        Analyze glyph composition patterns using n-gram statistics.
        """
        if not use_real_computation("stress_tests"):
            return 0.55  # Simulated

        if len(glyphs) < 2:
            return 0.0

        # Get glyph symbols
        symbols = []
        for glyph in glyphs:
            alignment = session.query(GlyphAlignmentRecord).filter_by(glyph_id=glyph.id).first()
            if alignment and alignment.symbol:
                symbols.append(alignment.symbol)
            else:
                # Use position-based proxy
                symbols.append(f"g{glyph.glyph_index}")

        if len(symbols) < 2:
            return 0.0

        # Count bigram types vs tokens
        bigrams = []
        for i in range(len(symbols) - 1):
            bigrams.append((symbols[i], symbols[i + 1]))

        bigram_types = len(set(bigrams))
        bigram_tokens = len(bigrams)

        # Type/token ratio: high ratio means more variety (less compositional)
        # Low ratio means repetitive patterns (more compositional)
        if bigram_tokens > 0:
            ttr = bigram_types / bigram_tokens
            # Invert: high compositional = low type/token ratio
            compositionality = 1.0 - min(1.0, ttr)
        else:
            compositionality = 0.0

        return compositionality

    def _test_procedural_signatures(self, session, dataset_id: str) -> Dict[str, Any]:
        """
        Test for procedural generation signatures.

        Generated text often shows:
        - Excessive regularity
        - Bounded state patterns
        - Deterministic-looking sequences

        Uses iteration limits for bounded runtime.
        """
        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()

        if not pages:
            return {"signature_strength": 0, "indicators": []}

        indicators = []

        # Test 1: Repetition patterns
        word_repetition = self._analyze_repetition_patterns(session, dataset_id)
        if word_repetition > 0.15:  # More than 15% exact repetition
            indicators.append("high_repetition")

        # Test 2: Sequence regularity
        regularity = self._analyze_sequence_regularity(session, dataset_id)
        if regularity > 0.7:
            indicators.append("excessive_regularity")

        # Test 3: State-based patterns
        state_boundedness = self._analyze_state_patterns(session, dataset_id)
        if state_boundedness > 0.6:
            indicators.append("bounded_states")

        # Calculate signature strength
        signature_strength = len(indicators) / 3.0

        return {
            "signature_strength": signature_strength,
            "indicators": indicators,
            "word_repetition": word_repetition,
            "regularity": regularity,
            "state_boundedness": state_boundedness,
        }

    def _analyze_repetition_patterns(self, session, dataset_id: str) -> float:
        """
        Analyze word/pattern repetition from actual token data.

        Uses MAX_TOKENS_ANALYZED limit for bounded runtime.
        """
        if not use_real_computation("stress_tests"):
            return 0.20  # Simulated

        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()
        if not pages:
            return 0.0

        page_ids = [p.id for p in pages]

        # Get tokens for this dataset with limit
        tokens = (
            session.query(TranscriptionTokenRecord.content)
            .join(TranscriptionLineRecord, TranscriptionTokenRecord.line_id == TranscriptionLineRecord.id)
            .filter(TranscriptionLineRecord.page_id.in_(page_ids))
            .limit(MAX_TOKENS_ANALYZED)
            .all()
        )

        if not tokens:
            return 0.0

        token_contents = [t[0] for t in tokens]
        token_counts = Counter(token_contents)

        total = len(token_contents)
        repeated = sum(c for c in token_counts.values() if c > 1)

        return repeated / total if total > 0 else 0.0

    def _analyze_sequence_regularity(self, session, dataset_id: str) -> float:
        """
        Analyze sequence regularity using n-gram entropy.

        Low entropy = high regularity (procedural)
        High entropy = low regularity (organic)

        Uses iteration limits for bounded runtime.
        """
        if not use_real_computation("stress_tests"):
            return 0.55  # Simulated

        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()
        if not pages:
            return 0.0

        page_ids = [p.id for p in pages]

        # Get tokens in order with limits
        all_tokens = []
        tokens_collected = 0

        for page_id in page_ids:
            if tokens_collected >= MAX_TOKENS_ANALYZED:
                break

            lines = (
                session.query(TranscriptionLineRecord)
                .filter_by(page_id=page_id)
                .order_by(TranscriptionLineRecord.line_index)
                .limit(MAX_LINES_PER_PAGE)
                .all()
            )

            for line in lines:
                if tokens_collected >= MAX_TOKENS_ANALYZED:
                    break

                tokens = (
                    session.query(TranscriptionTokenRecord)
                    .filter_by(line_id=line.id)
                    .order_by(TranscriptionTokenRecord.token_index)
                    .all()
                )
                for t in tokens:
                    if tokens_collected >= MAX_TOKENS_ANALYZED:
                        break
                    all_tokens.append(t.content)
                    tokens_collected += 1

        if len(all_tokens) < 10:
            return 0.0

        # Calculate bigram entropy
        bigrams = []
        for i in range(len(all_tokens) - 1):
            bigrams.append((all_tokens[i], all_tokens[i + 1]))

        bigram_counts = Counter(bigrams)
        total_bigrams = len(bigrams)

        if total_bigrams == 0:
            return 0.0

        # Shannon entropy
        entropy = 0.0
        for count in bigram_counts.values():
            p = count / total_bigrams
            if p > 0:
                entropy -= p * math.log2(p)

        # Normalize by max possible entropy
        max_entropy = math.log2(total_bigrams) if total_bigrams > 1 else 1

        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

        # Invert: regularity = 1 - normalized_entropy
        regularity = 1.0 - normalized_entropy

        return regularity

    def _analyze_state_patterns(self, session, dataset_id: str) -> float:
        """
        Analyze state-based transition patterns.

        Measures transition matrix sparsity (bounded states = sparse matrix).
        Uses iteration limits for bounded runtime.
        """
        if not use_real_computation("stress_tests"):
            return 0.50  # Simulated

        pages = session.query(PageRecord).filter_by(dataset_id=dataset_id).limit(MAX_PAGES_PER_TEST).all()
        if not pages:
            return 0.0

        page_ids = [p.id for p in pages]

        # Get tokens with limit
        tokens = (
            session.query(TranscriptionTokenRecord.content)
            .join(TranscriptionLineRecord, TranscriptionTokenRecord.line_id == TranscriptionLineRecord.id)
            .filter(TranscriptionLineRecord.page_id.in_(page_ids))
            .limit(MAX_TOKENS_ANALYZED)
            .all()
        )

        if len(tokens) < 10:
            return 0.0

        token_contents = [t[0] for t in tokens]
        unique_tokens = set(token_contents)
        vocab_size = len(unique_tokens)

        if vocab_size < 2:
            return 0.0

        # Count observed transitions
        transitions = set()
        for i in range(len(token_contents) - 1):
            transitions.add((token_contents[i], token_contents[i + 1]))

        # Maximum possible transitions
        max_transitions = vocab_size * vocab_size

        # Sparsity = 1 - (observed / possible)
        sparsity = 1.0 - (len(transitions) / max_transitions) if max_transitions > 0 else 0

        return sparsity

    def _analyze_results(self, explanation_class: str,
                         locality: Dict, compositionality: Dict,
                         procedural: Dict, controls: Dict) -> Dict[str, Any]:
        """Aggregate and analyze all results."""
        analysis = {
            "failures": [],
            "implications": [],
            "tightens_constraints": False,
        }

        # Determine dominant pattern type
        if locality.get("radius", 0) <= 4 and compositionality.get("score", 0) > 0.5:
            pattern_type = "local_compositional"
        elif procedural.get("signature_strength", 0) > 0.5:
            pattern_type = "procedural"
        elif locality.get("radius", 0) > 8:
            pattern_type = "global_dependent"
        else:
            pattern_type = "mixed"

        analysis["pattern_type"] = pattern_type

        # Class-specific analysis
        if explanation_class == "constructed_system":
            if procedural.get("signature_strength", 0) > 0.4:
                analysis["implications"].append(
                    "Procedural generation signatures detected; "
                    "consistent with constructed system hypothesis"
                )
            else:
                analysis["implications"].append(
                    "Weak procedural signatures; "
                    "constructed system less likely than organic production"
                )
                analysis["tightens_constraints"] = True

        elif explanation_class == "visual_grammar":
            if locality.get("radius", 0) <= 4:
                analysis["implications"].append(
                    "Strong locality supports visual grammar; "
                    "meaning depends on local spatial context"
                )
            else:
                analysis["failures"].append({
                    "type": "weak_locality",
                    "detail": "Visual grammar requires strong local dependencies"
                })
                analysis["tightens_constraints"] = True

        elif explanation_class == "hybrid_system":
            if pattern_type == "mixed":
                analysis["implications"].append(
                    "Mixed pattern type supports hybrid interpretation; "
                    "different sections may use different mechanisms"
                )
            else:
                analysis["implications"].append(
                    f"Dominant pattern ({pattern_type}) suggests single system, not hybrid"
                )
                analysis["tightens_constraints"] = True

        # Calculate overall score
        locality_score = min(1.0, locality.get("strength", 0) * 1.5)
        comp_score = compositionality.get("score", 0.5)
        proc_score = 1.0 - abs(0.5 - procedural.get("signature_strength", 0.5))

        analysis["overall_score"] = (locality_score + comp_score + proc_score) / 3

        # Control differential
        if controls:
            control_avg = sum(
                c["locality"].get("strength", 0)
                for c in controls.values()
            ) / len(controls)
            analysis["control_differential"] = locality.get("strength", 0) - control_avg
        else:
            analysis["control_differential"] = 0

        return analysis

    def _determine_outcome(self, explanation_class: str, analysis: Dict) -> StressTestOutcome:
        """Determine outcome based on analysis."""
        score = analysis.get("overall_score", 0.5)
        has_failures = len(analysis.get("failures", [])) > 0

        if has_failures:
            return StressTestOutcome.FRAGILE
        elif score > 0.6:
            return StressTestOutcome.STABLE
        elif score > 0.4:
            return StressTestOutcome.FRAGILE
        else:
            return StressTestOutcome.COLLAPSED

    def _generate_summary(self, explanation_class: str,
                          outcome: StressTestOutcome, analysis: Dict) -> str:
        """Generate human-readable summary."""
        pattern = analysis.get("pattern_type", "unknown")
        score = analysis.get("overall_score", 0.5)

        if outcome == StressTestOutcome.STABLE:
            return (
                f"{explanation_class}: Structure is {pattern.upper()} (score: {score:.2f}). "
                "Locality and compositionality patterns are consistent with hypothesis."
            )
        elif outcome == StressTestOutcome.FRAGILE:
            return (
                f"{explanation_class}: Structure shows {pattern.upper()} patterns (score: {score:.2f}). "
                "Some inconsistencies with hypothesis expectations."
            )
        elif outcome == StressTestOutcome.COLLAPSED:
            return (
                f"{explanation_class}: Structure patterns (score: {score:.2f}) "
                "inconsistent with hypothesis requirements."
            )
        else:
            return f"{explanation_class}: Results INCONCLUSIVE."
