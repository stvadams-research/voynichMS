{
  "provenance": {
    "run_id": "45b8ab47-e395-fec2-1984-d3edbba9b3fe",
    "git_commit": "0fad394e35b143e18d29478b5ef5b92ddd9fdbc4",
    "timestamp": "2026-02-10T16:50:35.150202+00:00",
    "seed": 2718,
    "experiment_id": "ea8741e3-5325-9767-3622-bf47f940ba43",
    "run_nonce": 1770742235150150000,
    "command": "run_proximity_uncertainty"
  },
  "results": {
    "target_artifact": "Voynich",
    "parameters": {
      "seed": 2718,
      "iterations": 4000,
      "dimension_count": 12,
      "bootstrap_dimensions": true,
      "weight_jitter_range": [
        0.85,
        1.15
      ],
      "jackknife_dimensions": true,
      "status_thresholds": {
        "min_nearest_neighbor_stability_for_confirmed": 0.75,
        "min_jackknife_stability_for_confirmed": 0.75,
        "min_rank_stability_for_confirmed": 0.65,
        "min_probability_margin_for_confirmed": 0.1,
        "min_top2_gap_ci_lower_for_confirmed": 0.05,
        "min_nearest_neighbor_stability_for_qualified": 0.5,
        "min_jackknife_stability_for_qualified": 0.7,
        "min_rank_stability_for_qualified": 0.55,
        "min_probability_margin_for_qualified": 0.03
      }
    },
    "status": "INCONCLUSIVE_UNCERTAINTY",
    "reason_code": "TOP2_GAP_FRAGILE",
    "allowed_claim": "Comparative claim remains provisional pending uncertainty-complete evidence.",
    "nearest_neighbor": "Lullian Wheels",
    "nearest_neighbor_distance": 5.0990195135927845,
    "nearest_neighbor_stability": 0.4485,
    "jackknife_nearest_neighbor_stability": 0.8333333333333334,
    "rank_stability": 0.4565,
    "rank_stability_components": {
      "top2_set_stability": 0.54275,
      "top3_set_stability": 0.4565,
      "full_ranking_match_rate": 0.0005
    },
    "nearest_neighbor_alternative": "Magic Squares",
    "nearest_neighbor_alternative_probability": 0.393,
    "nearest_neighbor_probability_margin": 0.055499999999999994,
    "distance_summary": {
      "Latin": {
        "mean": 8.40737440196734,
        "std": 1.1656515234245879,
        "ci95_lower": 6.060123779832564,
        "ci95_upper": 10.561086022510741,
        "point_estimate": 8.48528137423857,
        "nearest_probability": 0.01275,
        "top3_probability": 0.078,
        "rank_probability": {
          "1": 0.01275,
          "2": 0.02,
          "3": 0.04525,
          "4": 0.085,
          "5": 0.2775,
          "6": 0.1345,
          "7": 0.16775,
          "8": 0.15625,
          "9": 0.09675,
          "10": 0.00425
        },
        "expected_rank": 6.0605,
        "rank_entropy": 2.809999928980825
      },
      "Table-Grille": {
        "mean": 8.329142373433374,
        "std": 1.5788351320081393,
        "ci95_lower": 5.231997552097635,
        "ci95_upper": 11.334642713816482,
        "point_estimate": 8.426149773176359,
        "nearest_probability": 0.00075,
        "top3_probability": 0.131,
        "rank_probability": {
          "1": 0.00075,
          "2": 0.01675,
          "3": 0.1135,
          "4": 0.13025,
          "5": 0.18275,
          "6": 0.17025,
          "7": 0.132,
          "8": 0.22125,
          "9": 0.03075,
          "10": 0.00175
        },
        "expected_rank": 5.81925,
        "rank_entropy": 2.7665178888105033
      },
      "Magic Squares": {
        "mean": 5.380725874400034,
        "std": 1.5313549409363154,
        "ci95_lower": 2.4456617746561378,
        "ci95_upper": 8.327711922157643,
        "point_estimate": 5.5677643628300215,
        "nearest_probability": 0.393,
        "top3_probability": 0.87025,
        "rank_probability": {
          "1": 0.393,
          "2": 0.2855,
          "3": 0.19175,
          "4": 0.0635,
          "5": 0.0275,
          "6": 0.02525,
          "7": 0.0135
        },
        "expected_rank": 2.17675,
        "rank_entropy": 2.1157004205989267
      },
      "Vedic Chanting": {
        "mean": 8.351564328193929,
        "std": 1.4157527973393529,
        "ci95_lower": 5.625373971528857,
        "ci95_upper": 11.080841109248247,
        "point_estimate": 8.426149773176359,
        "nearest_probability": 0.0225,
        "top3_probability": 0.15275,
        "rank_probability": {
          "1": 0.0225,
          "2": 0.037,
          "3": 0.09325,
          "4": 0.10575,
          "5": 0.1315,
          "6": 0.15475,
          "7": 0.209,
          "8": 0.1005,
          "9": 0.13925,
          "10": 0.0065
        },
        "expected_rank": 5.9705,
        "rank_entropy": 3.010979888149487
      },
      "Codex Seraph.": {
        "mean": 7.858577645881675,
        "std": 1.1216439793806308,
        "ci95_lower": 5.603425298209923,
        "ci95_upper": 9.969598341769768,
        "point_estimate": 7.937253933193772,
        "nearest_probability": 0.00225,
        "top3_probability": 0.115,
        "rank_probability": {
          "1": 0.00225,
          "2": 0.016,
          "3": 0.09675,
          "4": 0.33425,
          "5": 0.08875,
          "6": 0.11025,
          "7": 0.12625,
          "8": 0.1235,
          "9": 0.09125,
          "10": 0.01075
        },
        "expected_rank": 5.56725,
        "rank_entropy": 2.7655937084717572
      },
      "Lingua Ignota": {
        "mean": 6.8611922777718926,
        "std": 1.9752639039082476,
        "ci95_lower": 2.975741188167715,
        "ci95_upper": 10.590238562105641,
        "point_estimate": 7.14142842854285,
        "nearest_probability": 0.10425,
        "top3_probability": 0.54925,
        "rank_probability": {
          "1": 0.10425,
          "2": 0.195,
          "3": 0.25,
          "4": 0.092,
          "5": 0.07625,
          "6": 0.06225,
          "7": 0.0475,
          "8": 0.06475,
          "9": 0.10725,
          "10": 0.00075
        },
        "expected_rank": 4.19025,
        "rank_entropy": 2.966854666345752
      },
      "Trithemius": {
        "mean": 8.534038545011358,
        "std": 1.562456604729272,
        "ci95_lower": 5.319182895206022,
        "ci95_upper": 11.505937218282565,
        "point_estimate": 8.660254037844387,
        "nearest_probability": 0.016,
        "top3_probability": 0.1455,
        "rank_probability": {
          "1": 0.016,
          "2": 0.0435,
          "3": 0.086,
          "4": 0.073,
          "5": 0.09325,
          "6": 0.1845,
          "7": 0.143,
          "8": 0.16075,
          "9": 0.17625,
          "10": 0.02375
        },
        "expected_rank": 6.337,
        "rank_entropy": 3.0359771370093522
      },
      "Lullian Wheels": {
        "mean": 4.891718027869493,
        "std": 1.4826773976489005,
        "ci95_lower": 2.458169166553182,
        "ci95_upper": 7.79288638125485,
        "point_estimate": 5.0990195135927845,
        "nearest_probability": 0.4485,
        "top3_probability": 0.9355,
        "rank_probability": {
          "1": 0.4485,
          "2": 0.38625,
          "3": 0.10075,
          "4": 0.03975,
          "5": 0.0125,
          "6": 0.0095,
          "7": 0.0025,
          "8": 0.00025
        },
        "expected_rank": 1.82125,
        "rank_entropy": 1.7349165196870862
      },
      "Penmanship": {
        "mean": 11.89191290059772,
        "std": 1.576074860735297,
        "ci95_lower": 8.498811075025158,
        "ci95_upper": 14.679829030917146,
        "point_estimate": 11.958260743101398,
        "nearest_probability": 0.0,
        "top3_probability": 0.0,
        "rank_probability": {
          "4": 0.00075,
          "5": 0.001,
          "6": 0.001,
          "7": 0.00775,
          "8": 0.0125,
          "9": 0.02825,
          "10": 0.94875
        },
        "expected_rank": 9.91,
        "rank_entropy": 0.3784546767357273
      },
      "Enochian": {
        "mean": 8.843784062074759,
        "std": 1.626136916417129,
        "ci95_lower": 5.497589877760527,
        "ci95_upper": 11.812832673979376,
        "point_estimate": 8.94427190999916,
        "nearest_probability": 0.0,
        "top3_probability": 0.02275,
        "rank_probability": {
          "3": 0.02275,
          "4": 0.07575,
          "5": 0.109,
          "6": 0.14775,
          "7": 0.15075,
          "8": 0.16025,
          "9": 0.33025,
          "10": 0.0035
        },
        "expected_rank": 7.14725,
        "rank_entropy": 2.553549534279976
      }
    },
    "ranking_point_estimate": [
      "Lullian Wheels",
      "Magic Squares",
      "Lingua Ignota",
      "Codex Seraph.",
      "Table-Grille",
      "Vedic Chanting",
      "Latin",
      "Trithemius",
      "Enochian",
      "Penmanship"
    ],
    "top2_gap": {
      "mean": 0.9582212908122134,
      "std": 0.7932715676781487,
      "ci95_lower": 0.022750524845312238,
      "ci95_upper": 2.992964364837489
    },
    "top2_gap_fragile": true,
    "metric_validity": {
      "required_fields_present": true,
      "missing_required_fields": [],
      "sufficient_iterations": true,
      "status_inputs": {
        "nearest_neighbor_stability": 0.4485,
        "jackknife_nearest_neighbor_stability": 0.8333333333333334,
        "rank_stability": 0.4565,
        "nearest_neighbor_probability_margin": 0.055499999999999994,
        "top2_gap_ci95_lower": 0.022750524845312238
      }
    }
  }
}