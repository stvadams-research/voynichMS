#!/usr/bin/env python3
"""Phase 14H: Baseline Showdown.

Compares the Lattice Model against three adversarial baselines using
proper two-part MDL: L(model) + L(data | model).

Each model gets its own generative likelihood for L(data | model),
ensuring the comparison reflects actual model fit, not just parameter count.
"""

import json
import math
import sys
from collections import Counter, defaultdict
from pathlib import Path

from rich.console import Console
from rich.table import Table

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from phase1_foundation.core.provenance import ProvenanceWriter
from phase1_foundation.core.queries import get_lines_from_store
from phase1_foundation.runs.manager import active_run
from phase1_foundation.storage.metadata import MetadataStore

DB_PATH = "sqlite:///data/voynich.db"
PALETTE_PATH = project_root / "results/data/phase14_machine/full_palette_grid.json"
OUTPUT_PATH = project_root / "results/data/phase14_machine/baseline_comparison.json"
console = Console()

BITS_PER_PARAM = 10  # Standard approximation for parameter encoding


def lattice_mdl(all_tokens, lines, lattice_map, window_contents):
    """
    Lattice MDL: tokens are generated by selecting from the current window.
    L(data|model) = sum of -log2(1 / |current_window|) for admissible tokens,
    plus a penalty for non-admissible tokens (full vocab cost).
    """
    num_windows = len(window_contents)
    vocab_size = len(set(all_tokens))
    fallback_cost = math.log2(vocab_size) if vocab_size > 1 else 1.0

    # L(model): encode lattice_map entries + window contents
    num_params = len(lattice_map) + sum(len(v) for v in window_contents.values())
    l_model = num_params * BITS_PER_PARAM

    # L(data | model): walk the lattice, cost each token
    l_data = 0.0
    current_window = 0

    for line in lines:
        for word in line:
            if word not in lattice_map:
                l_data += fallback_cost
                continue

            # Check current window and neighbors (drift = Â±1)
            found = False
            for offset in [-1, 0, 1]:
                check_win = (current_window + offset) % num_windows
                col = window_contents.get(check_win, [])
                if word in col:
                    l_data += math.log2(len(col)) if len(col) > 1 else 0.0
                    current_window = check_win
                    found = True
                    break

            if not found:
                l_data += fallback_cost
                current_window = lattice_map.get(word, (current_window + 1) % num_windows)
            else:
                current_window = lattice_map.get(word, (current_window + 1) % num_windows)

    return {"l_model": l_model, "l_data": l_data, "l_total": l_model + l_data}


def markov_o2_mdl(all_tokens, lines):
    """
    Markov Order-2 MDL: P(w_i | w_{i-1}, w_{i-2}) with add-1 smoothing.
    """
    vocab = set(all_tokens)
    vocab_size = len(vocab)

    # Count trigram and bigram contexts
    trigram_counts = defaultdict(Counter)
    bigram_counts = defaultdict(int)

    for line in lines:
        for i in range(2, len(line)):
            ctx = (line[i - 2], line[i - 1])
            trigram_counts[ctx][line[i]] += 1
            bigram_counts[ctx] += 1

    # L(model): one probability per observed (context, word) pair
    num_params = sum(len(v) for v in trigram_counts.values())
    l_model = num_params * BITS_PER_PARAM

    # L(data | model): conditional log-likelihood with add-1 smoothing
    l_data = 0.0
    fallback_cost = math.log2(vocab_size) if vocab_size > 1 else 1.0

    for line in lines:
        for i in range(len(line)):
            if i < 2:
                l_data += fallback_cost
            else:
                ctx = (line[i - 2], line[i - 1])
                total = bigram_counts.get(ctx, 0) + vocab_size  # add-1 denom
                count = trigram_counts.get(ctx, Counter()).get(line[i], 0) + 1  # add-1
                l_data += -math.log2(count / total)

    return {"l_model": l_model, "l_data": l_data, "l_total": l_model + l_data}


def copy_reset_mdl(all_tokens, lines):
    """
    Copy-Reset MDL: each token is either copied from the previous k tokens
    (with probability p_copy) or drawn from unigram distribution.
    Estimate p_copy and window k from data, then compute likelihood.
    """
    vocab_size = len(set(all_tokens))
    counts = Counter(all_tokens)
    total = len(all_tokens)
    unigram_logp = {w: math.log2(c / total) for w, c in counts.items()}
    fallback_cost = math.log2(vocab_size)

    # Estimate copy rate: how often does word match one of previous 5?
    copy_window = 5
    copy_hits = 0
    copy_eligible = 0
    for line in lines:
        for i in range(1, len(line)):
            copy_eligible += 1
            recent = set(line[max(0, i - copy_window):i])
            if line[i] in recent:
                copy_hits += 1

    p_copy = copy_hits / copy_eligible if copy_eligible > 0 else 0.0
    p_emit = 1.0 - p_copy

    # Clamp to avoid log(0)
    p_copy = max(p_copy, 1e-10)
    p_emit = max(p_emit, 1e-10)

    # L(model): 2 parameters (p_copy, copy_window)
    l_model = 2 * BITS_PER_PARAM

    # L(data | model)
    l_data = 0.0
    for line in lines:
        for i in range(len(line)):
            word = line[i]
            if i == 0:
                l_data += -unigram_logp.get(word, fallback_cost)
                continue

            recent = set(line[max(0, i - copy_window):i])
            if word in recent:
                # Cost = -log2(p_copy * 1/|recent|)
                l_data += -math.log2(p_copy / len(recent))
            else:
                # Cost = -log2(p_emit * P_unigram(word))
                ug = 2 ** unigram_logp.get(word, -fallback_cost)
                l_data += -math.log2(p_emit * max(ug, 1e-15))

    return {"l_model": l_model, "l_data": l_data, "l_total": l_model + l_data}


def table_grille_mdl(all_tokens, lines, num_cells=50):
    """
    Table-Grille MDL: text position (mod num_cells) determines which cell
    to draw from. Each cell has its own unigram distribution.
    """
    vocab_size = len(set(all_tokens))
    fallback_cost = math.log2(vocab_size) if vocab_size > 1 else 1.0

    # Assign tokens to cells by position
    cell_counts = defaultdict(Counter)
    cell_totals = defaultdict(int)
    pos = 0
    for line in lines:
        for word in line:
            cell = pos % num_cells
            cell_counts[cell][word] += 1
            cell_totals[cell] += 1
            pos += 1

    # L(model): one probability per (cell, word) pair
    num_params = sum(len(v) for v in cell_counts.values())
    l_model = num_params * BITS_PER_PARAM

    # L(data | model): -log2(P(word | cell)) with add-1 smoothing
    l_data = 0.0
    pos = 0
    for line in lines:
        for word in line:
            cell = pos % num_cells
            total = cell_totals[cell] + vocab_size
            count = cell_counts[cell].get(word, 0) + 1
            l_data += -math.log2(count / total)
            pos += 1

    return {"l_model": l_model, "l_data": l_data, "l_total": l_model + l_data}


def main():
    console.print("[bold red]Phase 14H: Baseline Showdown (Adversarial Comparison)[/bold red]")

    # 1. Load Real Data
    store = MetadataStore(DB_PATH)
    real_lines = get_lines_from_store(store, "voynich_real")
    all_tokens = [t for l in real_lines for t in l]
    total_tokens = len(all_tokens)
    console.print(f"Loaded {total_tokens} tokens across {len(real_lines)} lines.")

    # 2. Load Lattice
    with open(PALETTE_PATH, "r") as f:
        p_data = json.load(f)
    lattice_results = p_data.get("results", p_data)
    lattice_map = lattice_results.get("lattice_map", {})
    window_contents = {int(k): v for k, v in lattice_results.get("window_contents", {}).items()}

    # 3. Compute MDL for each model
    console.print("Computing Lattice MDL...")
    lat = lattice_mdl(all_tokens, real_lines, lattice_map, window_contents)

    console.print("Computing Markov-O2 MDL...")
    mar = markov_o2_mdl(all_tokens, real_lines)

    console.print("Computing Copy-Reset MDL...")
    cop = copy_reset_mdl(all_tokens, real_lines)

    console.print("Computing Table-Grille MDL...")
    tab = table_grille_mdl(all_tokens, real_lines)

    # 4. Report
    models = {
        "Lattice (Ours)": lat,
        "Markov-O2": mar,
        "Copy-Reset": cop,
        "Table-Grille": tab
    }

    results = {
        "total_tokens": total_tokens,
        "models": {}
    }
    for name, m in models.items():
        results["models"][name] = {
            "l_model": m["l_model"],
            "l_data": m["l_data"],
            "l_total": m["l_total"],
            "bpt": m["l_total"] / total_tokens
        }

    saved = ProvenanceWriter.save_results(results, OUTPUT_PATH)

    table = Table(title="Model Comparison (Two-Part MDL)")
    table.add_column("Model Family", style="cyan")
    table.add_column("L(model)", justify="right")
    table.add_column("L(data|model)", justify="right")
    table.add_column("L(total)", justify="right")
    table.add_column("BPT", justify="right", style="bold green")

    for name, data in results["models"].items():
        table.add_row(
            name,
            f"{data['l_model']:,.0f}",
            f"{data['l_data']:,.0f}",
            f"{data['l_total']:,.0f}",
            f"{data['bpt']:.4f}"
        )
    console.print(table)

    # Verdict
    lattice_bpt = results["models"]["Lattice (Ours)"]["bpt"]
    best_name = None
    best_bpt = float("inf")
    for name, data in results["models"].items():
        if "Lattice" not in name and data["bpt"] < best_bpt:
            best_bpt = data["bpt"]
            best_name = name

    if lattice_bpt < best_bpt:
        console.print(f"\n[bold green]SUCCESS:[/bold green] Lattice model is uniquely parsimonious "
                      f"(beats {best_name} by {best_bpt - lattice_bpt:.4f} BPT)")
    elif lattice_bpt == best_bpt:
        console.print(f"\n[bold yellow]TIE:[/bold yellow] Lattice model ties with {best_name}.")
    else:
        console.print(f"\n[bold yellow]GAP:[/bold yellow] {best_name} ({best_bpt:.4f} BPT) beats "
                      f"Lattice ({lattice_bpt:.4f} BPT). Delta: {lattice_bpt - best_bpt:.4f} BPT.")


if __name__ == "__main__":
    with active_run(config={"seed": 42, "command": "run_14h_baseline_showdown"}):
        main()
